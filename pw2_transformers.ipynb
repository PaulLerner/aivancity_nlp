{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PaulLerner/aivancity_nlp/blob/main/pw2_transformers.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a group for the Homework\n",
    "\n",
    "Before starting this Practical Work, make sure that you have a group (3 students) for the Homework. \n",
    "\n",
    "If you have trouble finding a group, please tell the teacher. If you do the homework alone without authorization, you will get 0/20.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and imports\n",
    "\n",
    "Hit `Ctrl+S` to save a copy of the Colab notebook to your drive\n",
    "\n",
    "Run on Google Colab GPU:\n",
    "- Connect\n",
    "- Modify execution\n",
    "- GPU\n",
    "\n",
    "![image.png](https://paullerner.github.io/aivancity_nlp/_static/colab_gpu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available(), \"Connect to GPU and try again (ask teacher for help)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGe4ha9NgfGy"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYj_37iZgjo9"
   },
   "source": [
    "Attention is a crucial component in the transformer, it allows to capture dependencies between different positions of two sequence of elements. In our case, and in most cases in NLP applications, sequences are sentences and elements are (sub)words.\n",
    "It is a powerful operation that allows to learn an alignment between each element in two sequences. It generates a score of how related each element in sequence1 and sequence2 are between each other.\n",
    "Understanding how attention works and being able to implement it are essential for anyone working with transformers. \n",
    "\n",
    "Given a query ($Q$), key ($K$), and value ($V$) tensors, the attention mechanism computes a weighted sum of the value tensor based on the similarity between the query and key tensors as shown in the following equation:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big)V\n",
    "$$\n",
    "\n",
    "where \n",
    "- $Q$ represents the query tensor.\n",
    "- $K$ represents the key tensor.\n",
    "- $V$ represents the value tensor.\n",
    "- $d_k$ represents the dimensionality of the key tensor.\n",
    "\n",
    "This is the image that was in the [original Transformer paper](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) and that shows the computations used in the attention.\n",
    "\n",
    "Forget about the right part, we'll get back to that later in the lab.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)\n",
    "\n",
    "\n",
    "In this exercise, we will dive into the attention mechanism. To do so, we are going to build a simple cross-attention function that we will then extend to a more complex multi-head self-attention module that incorporates the concept of causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0lBpBvtiEQX"
   },
   "source": [
    "## Building a Simple Self-Attention Function\n",
    "\n",
    "In self-attention, a single sequence acts as the query $Q$, key $K$, and value $V$, allowing attention to be computed within the sequence itself. This can be useful for syntactic where an attention head can model the relationship between part of speech like subjects and verbs. \n",
    "\n",
    "\n",
    "Given an input sequence $S$ and the transformation weights $W_Q$, $W_K$ and $W_V$, complete the `self_attention` function in the cell below. \n",
    "\n",
    "You need to implement the following:\n",
    "- Calculate the query, key, and value projections using linear transformations.\n",
    "- Compute the attention scores by performing the dot product between the query and key tensors.\n",
    "- Apply softmax activation to the attention scores to obtain the attention weights.\n",
    "- Multiply the attention weights with the value tensor to get the attended values.\n",
    "- Return the attended values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Matrix sizes\n",
    "\n",
    "- q: query size\n",
    "- d: hidden dimension\n",
    "- c: context length\n",
    "\n",
    "\n",
    "- Q: 1xqxd\n",
    "- K, V: 1xcxd\n",
    "- Q x K:  1xqxd x 1xsxd.T \n",
    "- (QK) x V:  1xqxs  x  V  1xsxd\n",
    "\n",
    "  \n",
    "- Attn: 1xqxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(S, W_Q, W_K, W_V):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m W_V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Value weights\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Perform self-attention\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m attended_values \u001b[38;5;241m=\u001b[39m \u001b[43mself_attention\u001b[49m(S, W_Q, W_K, W_V)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Expected output # 1,13, 2 (B, Sequence, Projection)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattended_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# Sequence\n",
    "S = torch.rand((1,13,3))\n",
    "\n",
    "# Projections\n",
    "W_Q = torch.rand((3, 2))  # Query weights\n",
    "W_K = torch.rand((3, 2))  # Key weights\n",
    "W_V = torch.rand((3, 2))  # Value weights\n",
    "\n",
    "# Perform self-attention\n",
    "attended_values = self_attention(S, W_Q, W_K, W_V)\n",
    "\n",
    "# Expected output # 1,13, 2 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = S @ W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = S @ W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = S @ W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 13])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2451)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0,0]@keys.transpose(1,2)[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4041, 0.2598])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.transpose(1,2)[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = queries @ keys.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = attention/(3**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention[0,0]/attention[0,0].sum()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0682, 0.0902, 0.0734, 0.0829, 0.0711, 0.0880, 0.0712, 0.0781, 0.0817,\n",
       "        0.0743, 0.0778, 0.0761, 0.0670])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention[0,0].exp()/attention[0,0].exp().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0682, 0.0902, 0.0734, 0.0829, 0.0711, 0.0880, 0.0712, 0.0781,\n",
       "          0.0817, 0.0743, 0.0778, 0.0761, 0.0670],\n",
       "         [0.0555, 0.1161, 0.0661, 0.0914, 0.0621, 0.1078, 0.0612, 0.0776,\n",
       "          0.0881, 0.0693, 0.0794, 0.0728, 0.0527],\n",
       "         [0.0636, 0.0986, 0.0710, 0.0860, 0.0679, 0.0946, 0.0678, 0.0781,\n",
       "          0.0841, 0.0727, 0.0785, 0.0752, 0.0618],\n",
       "         [0.0588, 0.1090, 0.0681, 0.0891, 0.0645, 0.1023, 0.0638, 0.0778,\n",
       "          0.0865, 0.0707, 0.0793, 0.0738, 0.0563],\n",
       "         [0.0656, 0.0946, 0.0723, 0.0847, 0.0692, 0.0916, 0.0695, 0.0783,\n",
       "          0.0831, 0.0734, 0.0779, 0.0758, 0.0641],\n",
       "         [0.0581, 0.1113, 0.0672, 0.0893, 0.0643, 0.1037, 0.0628, 0.0773,\n",
       "          0.0867, 0.0705, 0.0801, 0.0732, 0.0554],\n",
       "         [0.0676, 0.0925, 0.0725, 0.0831, 0.0709, 0.0894, 0.0701, 0.0775,\n",
       "          0.0819, 0.0742, 0.0789, 0.0755, 0.0660],\n",
       "         [0.0607, 0.1046, 0.0694, 0.0880, 0.0658, 0.0992, 0.0655, 0.0781,\n",
       "          0.0856, 0.0715, 0.0789, 0.0745, 0.0585],\n",
       "         [0.0592, 0.1078, 0.0684, 0.0889, 0.0648, 0.1016, 0.0642, 0.0779,\n",
       "          0.0863, 0.0709, 0.0791, 0.0740, 0.0568],\n",
       "         [0.0660, 0.0953, 0.0717, 0.0842, 0.0698, 0.0916, 0.0690, 0.0776,\n",
       "          0.0828, 0.0736, 0.0790, 0.0752, 0.0642],\n",
       "         [0.0648, 0.0977, 0.0710, 0.0850, 0.0690, 0.0934, 0.0680, 0.0775,\n",
       "          0.0834, 0.0732, 0.0793, 0.0749, 0.0628],\n",
       "         [0.0642, 0.0986, 0.0707, 0.0853, 0.0686, 0.0942, 0.0677, 0.0776,\n",
       "          0.0836, 0.0730, 0.0794, 0.0748, 0.0623],\n",
       "         [0.0688, 0.0891, 0.0737, 0.0825, 0.0715, 0.0871, 0.0717, 0.0780,\n",
       "          0.0813, 0.0745, 0.0777, 0.0763, 0.0677]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(attention,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3566)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(attention,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =  attention@values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0707, 0.0846, 0.0700, 0.0651, 0.0881, 0.0789, 0.0915, 0.0573, 0.0748,\n",
       "        0.0798, 0.0879, 0.0791, 0.0722])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0,0].exp()/attention[0,0].exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6989, 1.5733, 0.9340, 0.7356, 0.9633, 0.9619, 1.1232, 0.8493, 0.8353,\n",
       "         0.9261, 1.1123, 1.5173, 0.8732]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attention/attention.sum(2)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5187, 0.8822],\n",
       "         [0.6730, 0.3559],\n",
       "         [0.2996, 0.6888],\n",
       "         [0.3598, 0.6056],\n",
       "         [0.7750, 0.9924],\n",
       "         [0.4920, 0.8769],\n",
       "         [0.6193, 0.9817],\n",
       "         [0.1077, 0.2411],\n",
       "         [0.4771, 0.8975],\n",
       "         [0.7294, 0.7808],\n",
       "         [0.4628, 1.0128],\n",
       "         [0.6518, 0.2787],\n",
       "         [0.5306, 0.6791]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head\n",
    "\n",
    "However, the relations present even in a single sentence are more than one. Think about number and gender agreement as one, the semantic relation between subject and object, the functional aspect that verb arguments have etc. All this cannot be modeled by a single head.\n",
    "\n",
    "For this reason, we are going to extend the single-head attention function to **multi-head attention**. In the previous implementation, we had one set of weights for the input query, resulting in a single type of _relationship between the the source and target sequence_. With multi-head attention, we can utilize _multiple parallel single-head attention modules_ to obtain diverse relationships between the query and the values. The attention operation works by projecting the sequences through a multiplication with a projection matrix, and then computing the alignment score. These are are all operation that can be parallelized since there's no interdependency between each each head. For this reasons, each head could learn to model a different linguistic intereation useful for many downstream tasks, be it syntactic, semantic or generation-based..\n",
    "\n",
    "\n",
    "As we've seen in class, this can be done simply by reshaping queries, keys and values.\n",
    "\n",
    "Project back the results using $W_O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(S, W_Q, W_K, W_V, W_O, num_heads=4):    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_head_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m W_O \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))  \u001b[38;5;66;03m# Output proj\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Perform self-attention\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m attended_values \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_attention\u001b[49m(S, W_Q, W_K, W_V, W_O)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Expected output # 1, 13, 8 (B, Sequence, Projection)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattended_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_head_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# Sequence\n",
    "S = torch.rand((1,13,8))\n",
    "\n",
    "# Projections\n",
    "W_Q = torch.rand((8, 8))  # Query weights\n",
    "W_K = torch.rand((8, 8))  # Key weights\n",
    "W_V = torch.rand((8, 8))  # Value weights\n",
    "W_O = torch.rand((8, 8))  # Output proj\n",
    "\n",
    "# Perform self-attention\n",
    "attended_values = multi_head_attention(S, W_Q, W_K, W_V, W_O)\n",
    "\n",
    "# Expected output # 1, 13, 8 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = S@W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 8])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = S@W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_heads=queries.reshape(1,13,4,2).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 13, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_heads=keys.reshape(1,13,4,2).transpose(1,2).transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 13])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6037)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_heads[0,0,0]@k_heads[0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7974, 1.9794])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_heads[0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 13, 13])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q_heads@k_heads).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.6037,  6.2601,  4.5226,  8.0496,  6.2530,  5.1778,  7.4772,\n",
       "            6.6409,  4.9655,  7.1913,  5.9260,  5.2925,  7.8081],\n",
       "          [ 6.6336,  5.4431,  3.8627,  7.0310,  5.4113,  4.4532,  6.5063,\n",
       "            5.8138,  4.3053,  6.2762,  5.1164,  4.5346,  6.8003],\n",
       "          [ 4.8231,  4.0273,  3.1239,  5.0802,  4.1016,  3.4811,  4.7952,\n",
       "            4.1504,  3.2319,  4.5543,  3.9237,  3.6113,  4.9884],\n",
       "          [ 6.9348,  5.7678,  4.3887,  7.3150,  5.8428,  4.9258,  6.8736,\n",
       "            5.9927,  4.6138,  6.5513,  5.5752,  5.0898,  7.1582],\n",
       "          [ 7.7639,  6.4461,  4.8624,  8.1946,  6.5143,  5.4754,  7.6849,\n",
       "            6.7215,  5.1490,  7.3360,  6.2088,  5.6476,  8.0068],\n",
       "          [ 6.2189,  5.1597,  3.8783,  6.5656,  5.2092,  4.3731,  6.1522,\n",
       "            5.3879,  4.1190,  5.8766,  4.9626,  4.5073,  6.4112],\n",
       "          [ 6.3159,  5.2413,  3.9438,  6.6675,  5.2932,  4.4452,  6.2493,\n",
       "            5.4708,  4.1849,  5.9682,  5.0432,  4.5826,  6.5119],\n",
       "          [ 5.7619,  4.8075,  3.7151,  6.0709,  4.8910,  4.1457,  5.7251,\n",
       "            4.9625,  3.8556,  5.4414,  4.6766,  4.2974,  5.9571],\n",
       "          [ 4.5076,  3.7804,  2.9943,  4.7404,  3.8729,  3.3110,  4.4969,\n",
       "            3.8608,  3.0446,  4.2543,  3.7153,  3.4495,  4.6726],\n",
       "          [ 6.0922,  5.0515,  3.7853,  6.4333,  5.0957,  4.2732,  6.0240,\n",
       "            5.2816,  4.0306,  5.7573,  4.8525,  4.4015,  6.2786],\n",
       "          [ 4.6068,  3.8394,  2.9507,  4.8558,  3.9000,  3.2994,  4.5733,\n",
       "            3.9724,  3.0763,  4.3510,  3.7263,  3.4163,  4.7601],\n",
       "          [ 5.0216,  4.1746,  3.1691,  5.2978,  4.2262,  3.5601,  4.9755,\n",
       "            4.3416,  3.3381,  4.7442,  4.0314,  3.6768,  5.1821],\n",
       "          [ 8.8431,  7.3402,  5.5296,  9.3346,  7.4152,  6.2298,  8.7513,\n",
       "            7.6579,  5.8619,  8.3559,  7.0662,  6.4239,  9.1186]],\n",
       "\n",
       "         [[ 9.7839, 10.2542,  6.3499,  9.7390,  8.6433,  8.4177,  9.1020,\n",
       "            6.9804,  5.8178,  7.7371,  8.6309,  8.5963, 10.8770],\n",
       "          [ 8.0000,  8.3761,  5.3086,  8.0127,  7.1138,  6.9238,  7.3600,\n",
       "            5.7411,  4.8977,  6.4256,  7.0945,  7.1339,  8.8692],\n",
       "          [ 5.2285,  5.4740,  3.4731,  5.2383,  4.6507,  4.5263,  4.8076,\n",
       "            3.7532,  3.2053,  4.2026,  4.6378,  4.6657,  5.7958],\n",
       "          [ 8.4252,  8.8256,  5.5306,  8.4130,  7.4679,  7.2706,  7.7936,\n",
       "            6.0289,  5.0854,  6.7159,  7.4523,  7.4589,  9.3532],\n",
       "          [ 8.0041,  8.3904,  5.1742,  7.9587,  7.0628,  6.8792,  7.4608,\n",
       "            5.7047,  4.7346,  6.3121,  7.0543,  7.0140,  8.9027],\n",
       "          [ 7.1235,  7.4653,  4.6325,  7.0947,  6.2967,  6.1320,  6.6205,\n",
       "            5.0850,  4.2470,  5.6411,  6.2870,  6.2671,  7.9174],\n",
       "          [ 6.8764,  7.1942,  4.6382,  6.9191,  6.1445,  5.9776,  6.2730,\n",
       "            4.9563,  4.3006,  5.5872,  6.1220,  6.1997,  7.6076],\n",
       "          [ 6.1499,  6.4421,  4.0370,  6.1410,  5.4511,  5.3071,  5.6889,\n",
       "            4.4007,  3.7120,  4.9022,  5.4397,  5.4445,  6.8273],\n",
       "          [ 5.0463,  5.2872,  3.2978,  5.0327,  4.4670,  4.3496,  4.6785,\n",
       "            3.6068,  3.0281,  4.0099,  4.4589,  4.4542,  5.6053],\n",
       "          [ 7.9774,  8.3583,  5.2129,  7.9558,  7.0615,  6.8759,  7.3963,\n",
       "            5.7017,  4.7864,  6.3387,  7.0487,  7.0410,  8.8612],\n",
       "          [ 5.9062,  6.1799,  3.9736,  5.9385,  5.2735,  5.1307,  5.3951,\n",
       "            4.2541,  3.6815,  4.7902,  5.2550,  5.3158,  6.5364],\n",
       "          [ 5.7169,  5.9757,  3.9298,  5.7836,  5.1377,  4.9955,  5.1630,\n",
       "            4.1417,  3.6643,  4.7078,  5.1132,  5.2207,  6.3092],\n",
       "          [ 9.4212,  9.8722,  6.1388,  9.3882,  8.3326,  8.1142,  8.7473,\n",
       "            6.7286,  5.6316,  7.4710,  8.3187,  8.2996, 10.4686]],\n",
       "\n",
       "         [[ 8.7714,  9.2132,  5.9843,  8.6848,  8.4672,  7.1160,  8.2667,\n",
       "            7.1703,  5.9611,  7.3026,  7.2703,  7.2138,  9.2880],\n",
       "          [ 7.9260,  8.2008,  5.5143,  7.7703,  7.6507,  6.4237,  7.5034,\n",
       "            6.4234,  5.3613,  6.6475,  6.5327,  6.5603,  8.3221],\n",
       "          [ 7.1398,  7.3327,  5.0144,  6.9655,  6.8916,  5.7837,  6.7739,\n",
       "            5.7617,  4.8185,  6.0096,  5.8684,  5.9280,  7.4656],\n",
       "          [ 9.4172,  9.7747,  6.5253,  9.2515,  9.0902,  7.6339,  8.9068,\n",
       "            7.6457,  6.3763,  7.8861,  7.7709,  7.7842,  9.9054],\n",
       "          [ 9.4138,  9.7676,  6.5259,  9.2459,  9.0869,  7.6309,  8.9046,\n",
       "            7.6414,  6.3733,  7.8847,  7.7671,  7.7826,  9.8999],\n",
       "          [ 7.7781,  8.0765,  5.3868,  7.6432,  7.5080,  6.3053,  7.3556,\n",
       "            6.3164,  5.2671,  6.5122,  6.4193,  6.4282,  8.1831],\n",
       "          [ 7.6772,  7.9264,  5.3558,  7.5158,  7.4104,  6.2212,  7.2724,\n",
       "            6.2141,  5.1896,  6.4455,  6.3225,  6.3600,  8.0512],\n",
       "          [ 7.9374,  8.1899,  5.5417,  7.7673,  7.6616,  6.4318,  7.5203,\n",
       "            6.4224,  5.3644,  6.6660,  6.5353,  6.5773,  8.3212],\n",
       "          [ 7.5039,  7.6876,  5.2864,  7.3089,  7.2430,  6.0777,  7.1245,\n",
       "            6.0470,  5.0604,  6.3235,  6.1621,  6.2367,  7.8355],\n",
       "          [ 8.3310,  8.6952,  5.7314,  8.2142,  8.0419,  6.7558,  7.8665,\n",
       "            6.7854,  5.6505,  6.9577,  6.8888,  6.8702,  8.7901],\n",
       "          [ 7.4467,  7.6016,  5.2697,  7.2361,  7.1877,  6.0300,  7.0776,\n",
       "            5.9886,  5.0162,  6.2861,  6.1070,  6.1983,  7.7603],\n",
       "          [ 7.2798,  7.4649,  5.1226,  7.0949,  7.0267,  5.8965,  6.9098,\n",
       "            5.8695,  4.9106,  6.1320,  5.9801,  6.0481,  7.6054],\n",
       "          [ 9.7325, 10.1960,  6.6628,  9.6198,  9.3948,  7.8943,  9.1796,\n",
       "            7.9439,  6.6088,  8.1132,  8.0590,  8.0132, 10.2904]],\n",
       "\n",
       "         [[ 7.6398,  7.3072,  3.3747,  7.2426,  5.4272,  4.5174,  7.3667,\n",
       "            5.8656,  3.7607,  6.3076,  5.5559,  4.7272,  6.9701],\n",
       "          [ 7.7689,  7.4385,  3.4317,  7.3521,  5.5278,  4.5827,  7.4872,\n",
       "            5.9708,  3.8288,  6.4123,  5.6372,  4.7923,  7.0667],\n",
       "          [ 6.5305,  6.2491,  2.8847,  6.1860,  4.6425,  3.8573,  6.2955,\n",
       "            5.0162,  3.2164,  5.3910,  4.7444,  4.0352,  5.9500],\n",
       "          [ 8.0995,  7.7455,  3.5778,  7.6808,  5.7521,  4.7913,  7.8107,\n",
       "            6.2174,  3.9861,  6.6875,  5.8926,  5.0144,  7.3935],\n",
       "          [ 7.1355,  6.8216,  3.1520,  6.7701,  5.0652,  4.2241,  6.8822,\n",
       "            5.4758,  3.5104,  5.8921,  5.1946,  4.4215,  6.5192],\n",
       "          [ 6.7378,  6.4391,  2.9763,  6.3967,  4.7803,  3.9920,  6.4998,\n",
       "            5.1688,  3.3134,  5.5643,  4.9088,  4.1795,  6.1622],\n",
       "          [ 9.1714,  8.7970,  4.0511,  8.6528,  6.5438,  5.3872,  8.8305,\n",
       "            7.0610,  4.5295,  7.5660,  6.6291,  5.6272,  8.2988],\n",
       "          [ 6.2388,  5.9707,  2.7558,  5.9085,  4.4360,  3.6839,  6.0139,\n",
       "            4.7927,  3.0731,  5.1500,  4.5313,  3.8535,  5.6822],\n",
       "          [ 5.0986,  4.8698,  2.2523,  4.8452,  3.6141,  3.0249,  4.9200,\n",
       "            3.9091,  2.5056,  4.2113,  3.7192,  3.1681,  4.6709],\n",
       "          [ 6.9226,  6.6201,  3.0579,  6.5646,  4.9164,  4.0950,  6.6757,\n",
       "            5.3140,  3.4069,  5.7157,  5.0362,  4.2856,  6.3189],\n",
       "          [ 7.7033,  7.3750,  3.4027,  7.2911,  5.4804,  4.5450,  7.4243,\n",
       "            5.9198,  3.7961,  6.3584,  5.5907,  4.7532,  7.0089],\n",
       "          [ 8.5876,  8.2251,  3.7933,  8.1220,  6.1135,  5.0615,  8.2746,\n",
       "            6.6021,  4.2340,  7.0873,  6.2266,  5.2919,  7.8035],\n",
       "          [ 8.8315,  8.4451,  3.9011,  8.3755,  6.2716,  5.2249,  8.5168,\n",
       "            6.7790,  4.3461,  7.2920,  6.4257,  5.4682,  8.0626]]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_heads@k_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 13, 13])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(@).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2, 13])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.reshape(1,13,4,2).transpose(1,2).transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 4, 4])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(queries.reshape(1,13,4,2)@keys.reshape(1,13,4,2).transpose(2,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 2, 4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.reshape(1,13,4,2).transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(queries@keys.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Mask\n",
    "\n",
    "GPT uses a version of self-attention called causal self-attention. When training our models for tasks like language modeling and machine translation, in practice we feed the entire train sequence to the model but, at every timestep, we want to prevent it to compute the alignment with future tokens. For this reason we use a mask that we incrementally lift at every timestep. For instance, we have a sentence that says \"Libson is a great city to live in\". At time 0, we feed the entire sentence to the model masking everything but the first token. Using the strikethrough format as masking, this will be what the model sees at step 0:\n",
    "\n",
    "- Time 0: Libson ~is a great city to live in~\n",
    "\n",
    "We then let the model generate a token a and move to step 1 where we are masking everything but the first two tokens\n",
    " \n",
    "- Time 1: Libson is ~a great city to live in~ \n",
    "\n",
    "and so on...\n",
    "\n",
    "- Time 2: Libson is a ~great city to live in~ \n",
    "- Time 3: Libson is a great ~city to live in~ \n",
    "- Time 4: Libson is a great city ~to live in~ \n",
    "- Time 5: Libson is a great city to ~live in~ \n",
    "- Time 6: Libson is a great city to live ~in~ \n",
    "\n",
    "\n",
    "![transformer](https://paullerner.github.io/aivancity_nlp/_static/attention_mask.png)\n",
    "\n",
    "Apply mask on attention using `torch.tril` and `masked_fill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_multi_head_attention(S, W_Q, W_K, W_V, W_O, num_heads=4):    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional attention:\n",
      "tensor([[[[ 7.6750,  9.8763,  7.8718],\n",
      "          [11.7096, 14.9262, 11.9657],\n",
      "          [ 8.8149, 11.3644,  9.0476]],\n",
      "\n",
      "         [[ 6.9848,  8.4330,  6.1530],\n",
      "          [ 8.9807, 10.8143,  7.9368],\n",
      "          [ 7.9229,  9.5573,  6.9868]],\n",
      "\n",
      "         [[ 8.4009, 10.4845,  9.2640],\n",
      "          [10.3813, 12.9258, 11.4504],\n",
      "          [ 8.6193, 10.7093,  9.5088]],\n",
      "\n",
      "         [[ 9.4824, 11.5997, 10.2568],\n",
      "          [13.4200, 16.4683, 14.5326],\n",
      "          [10.5152, 12.8738, 11.3774]]]])\n",
      "\n",
      "causal attention:\n",
      "tensor([[[[ 7.6750,    -inf,    -inf],\n",
      "          [11.7096, 14.9262,    -inf],\n",
      "          [ 8.8149, 11.3644,  9.0476]],\n",
      "\n",
      "         [[ 6.9848,    -inf,    -inf],\n",
      "          [ 8.9807, 10.8143,    -inf],\n",
      "          [ 7.9229,  9.5573,  6.9868]],\n",
      "\n",
      "         [[ 8.4009,    -inf,    -inf],\n",
      "          [10.3813, 12.9258,    -inf],\n",
      "          [ 8.6193, 10.7093,  9.5088]],\n",
      "\n",
      "         [[ 9.4824,    -inf,    -inf],\n",
      "          [13.4200, 16.4683,    -inf],\n",
      "          [10.5152, 12.8738, 11.3774]]]])\n",
      "Output Shape: torch.Size([1, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "# Sequence\n",
    "S = torch.rand((1,3,8))\n",
    "\n",
    "# Projections\n",
    "W_Q = torch.rand((8, 8))  # Query weights\n",
    "W_K = torch.rand((8, 8))  # Key weights\n",
    "W_V = torch.rand((8, 8))  # Value weights\n",
    "W_O = torch.rand((8, 8))  # Output proj\n",
    "\n",
    "# Perform self-attention\n",
    "attended_values = causal_multi_head_attention(S, W_Q, W_K, W_V, W_O)\n",
    "\n",
    "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now look back at the attention figure from the paper. Hopefully, you are now able to understand also the right side of the figure.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs7ar9XJu0u6"
   },
   "source": [
    "## Pytorch Module\n",
    "\n",
    "The last modification involves embedding our function into a PyTorch module. As you may have noticed, in the previous exercise, we passed the transformation weights as inputs to the function. In a real-world scenario, these matrices are learned, and PyTorch can keep track of them for us.\n",
    "\n",
    "- Complete the missing lines on the initialization of the module and the forward pass.\n",
    "- add dropout on the attention weights and the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_module = CausalSelfAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([1, 3, 8])\n"
     ]
    }
   ],
   "source": [
    "# Sequence\n",
    "S = torch.rand((1,3,8))\n",
    "\n",
    "# Perform self-attention\n",
    "attended_values = attention_module(S)\n",
    "\n",
    "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "\n",
    "![transformer](https://paullerner.github.io/aivancity_nlp/_static/transformer_decoder.png)\n",
    "\n",
    "## Attention is almost all you need: feedforward neural network\n",
    "\n",
    "Simple Neural network of two layers with a ReLU activation in-between and dropout at output. The intermediate dimension should be 4 times `hidden_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "- stack CausalSelfAttention and FeedForward\n",
    "- add residual connections\n",
    "- add layer norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence\n",
    "S = torch.rand((1,3,8))\n",
    "\n",
    "block = Block()\n",
    "# Perform self-attention\n",
    "output = block(S)\n",
    "\n",
    "# Expected output # 1, 3, 8 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Transformer\n",
    "- word embeddings\n",
    "- position embeddings\n",
    "- as many blocks as you like to stack\n",
    "- output layer back to the vocabulary (no need for softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=100, hidden_size=8, num_heads=2, dropout=0.1, seq_len=3, num_layers=2):\n",
    "        super().__init__()        \n",
    "            \n",
    "    def forward(self, input_ids):\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0,100,(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[85, 10,  4]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = transformer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 100])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores (not probabilities because not normalized) over the complete vocabulary, for each token in the sentence\n",
    "# shape: batch size, seq_len, V\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRiFro12JhO3"
   },
   "source": [
    "# Training\n",
    "\n",
    "A peak into Language Modeling (next class)\n",
    "\n",
    "\n",
    "![lm](https://paullerner.github.io/aivancity_nlp/_static/lm.png)\n",
    "\n",
    "A language model estimates the probability of a sequence of words $w$:\n",
    "$$P(w)=\\prod_t^{|w|} P(w_t | w_{<t}) = P(w_1)  P(w_2|w_1)  P(w_3 | w_1 w_2)...$$\n",
    "\n",
    "See how this turns into a sequence of classification problem:\n",
    "- first $P(w_1)$\n",
    "- then $P(w_2|w_1)$\n",
    "- etc.\n",
    "\n",
    "The model \"predicts the next word\" given a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniforge3/envs/matos/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 4358\n",
      "train 1801350\n",
      "validation 3760\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "dataset = {k: v[\"text\"] for k, v in dataset.items()}\n",
    "\n",
    "for k, v in dataset.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization\n",
    "\n",
    "We almost did not talk about tokenization yet! We've assumed words, which is impractical given finite vocabulary size.\n",
    "\n",
    "Instead, LLMs rely on BPE, a data compression technique, which segments rare words into subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "seq_len=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠSen', 'j', 'Åį', 'Ġno', 'ĠV', 'alky', 'ria', 'Ġ3', 'Ġ:', 'ĠUn', 'recorded', 'ĠChronicles', 'Ġ(', 'ĠJapanese', 'Ġ:', 'Ġæ', 'Ī', '¦', 'å', 'ł', '´', 'ãģ®', 'ãĥ´ãĤ¡', 'ãĥ«', 'ãĤŃ', 'ãĥ¥', 'ãĥª', 'ãĤ¢', '3', 'Ġ,', 'Ġlit', 'Ġ.', 'ĠV', 'alky', 'ria', 'Ġof', 'Ġthe', 'ĠBattlefield', 'Ġ3', 'Ġ)', 'Ġ,', 'Ġcommonly', 'Ġreferred', 'Ġto', 'Ġas', 'ĠV', 'alky', 'ria', 'ĠChronicles', 'ĠIII', 'Ġoutside', 'ĠJapan', 'Ġ,', 'Ġis', 'Ġa', 'Ġtactical', 'Ġrole', 'Ġ@', '-', '@', 'Ġplaying', 'Ġvideo', 'Ġgame', 'Ġdeveloped', 'Ġby', 'ĠSega', 'Ġand', 'ĠMedia', '.', 'Vision', 'Ġfor', 'Ġthe', 'ĠPlayStation', 'ĠPortable', 'Ġ.', 'ĠReleased', 'Ġin', 'ĠJanuary', 'Ġ2011', 'Ġin', 'ĠJapan', 'Ġ,', 'Ġit', 'Ġis', 'Ġthe', 'Ġthird', 'Ġgame', 'Ġin', 'Ġthe', 'ĠV', 'alky', 'ria', 'Ġseries', 'Ġ.', 'ĠEmploy', 'ing', 'Ġthe', 'Ġsame', 'Ġfusion', 'Ġof', 'Ġtactical', 'Ġand', 'Ġreal', 'Ġ@', '-', '@', 'Ġtime', 'Ġgameplay', 'Ġas', 'Ġits', 'Ġpredecessors', 'Ġ,', 'Ġthe', 'Ġstory', 'Ġruns', 'Ġparallel', 'Ġto', 'Ġthe', 'Ġfirst', 'Ġgame', 'Ġand', 'Ġfollows', 'Ġthe', 'Ġ\"', 'ĠNam', 'eless', 'Ġ\"', 'Ġ,', 'Ġa', 'Ġpenal', 'Ġmilitary', 'Ġunit', 'Ġserving', 'Ġthe', 'Ġnation', 'Ġof', 'ĠGall', 'ia', 'Ġduring', 'Ġthe', 'ĠSecond', 'ĠEuro', 'pan', 'ĠWar', 'Ġwho', 'Ġperform', 'Ġsecret', 'Ġblack', 'Ġoperations', 'Ġand', 'Ġare', 'Ġpitted', 'Ġagainst', 'Ġthe', 'ĠImperial', 'Ġunit', 'Ġ\"', 'ĠCal', 'am', 'at', 'y', 'ĠRaven', 'Ġ\"', 'Ġ.', 'Ġ', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(dataset[\"train\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = dataset[\"train\"][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface's `transformers` provides a convenient way to tokenize text, it also takes care of padding the text so that we can wrap all examples of a batch in the same `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [ 2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
       "         47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
       "           112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
       "          6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
       "           837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
       "          2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
       "          2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
       "           329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
       "          2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
       "         18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
       "         16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
       "         27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
       "           290,  5679,   262,   366, 17871,  5321,   366,   837]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the padding: small texts are padded by `tokenizer.eos_token_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size=tokenizer.vocab_size, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 50257])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as before (only larger seq_len and V)\n",
    "logits = transformer(input_ids)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-supervision\n",
    "\n",
    "Remember the greatest thing about Language Modeling: we don't need to annotate data!\n",
    "\n",
    "The model should predict the next word given the context so we just need to shift the input by 1 to get the labels!\n",
    "\n",
    "Compute the loss on one batch using `nn.CrossEntropyLoss`. Be careful about the padding! We don't want our model to learn to predict padding at the end of text!\n",
    "\n",
    "Like in the previous Practical Work, remember to flatten the batch dimension with the sequence dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0828, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss of randomly initialized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything about this value? What about its exponentiate? Ever heard of perplexity? More about this in the next class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Ensure that everything is on GPU by calling `.cuda()` or passing `device=\"cuda\"` on init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tensorboard before training. Refresh during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=128\n",
    "transformer = Transformer(vocab_size=tokenizer.vocab_size, hidden_size=256, num_layers=3, num_heads=4, dropout=0.1, seq_len=seq_len).cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0001)\n",
    "\n",
    "batch_size = 32\n",
    "# in the interest of time, we simply overfit on a single batch\n",
    "# try to train on the complete texts when you have more time\n",
    "train_loader = torch.utils.data.DataLoader(dataset[\"train\"][:batch_size], batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "steps = 0\n",
    "for epoch in range(1000):\n",
    "    for text_batch in train_loader:\n",
    "        input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
    "        logits = transformer(input_ids)\n",
    "        raise NotImplementedError(\"TODO compute loss\")\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), steps)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "\n",
    "        # validation\n",
    "        if steps % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                transformer.eval()\n",
    "                valid_loss = 0\n",
    "                valid_batches = 0\n",
    "                for text_batch in validation_loader:\n",
    "                    input_ids = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
    "                    logits = transformer(input_ids)\n",
    "                    raise NotImplementedError(\"TODO compute loss\")\n",
    "                    valid_loss += loss.item()\n",
    "                    valid_batches += 1\n",
    "                transformer.train()\n",
    "                writer.add_scalar(\"Loss/validation\", valid_loss/valid_batches, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"transformer.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode(model, input_ids, max_new_tokens=32):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # get the predictions\n",
    "        logits = model(input_ids)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1] # becomes (B, C)\n",
    "        # greedy decoding\n",
    "        idx_next = logits.argmax(1).unsqueeze(0)\n",
    "        # append sampled index to the running sequence\n",
    "        input_ids = torch.cat((input_ids, idx_next), dim=1) # (B, T+1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load previously saved model\n",
    "#transformer.load_state_dict(torch.load(\"transformer.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When overfitting on one single batch, the model simply memorizes training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" As with previous\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer([prompt], return_tensors='pt', padding=True, truncation=True, max_length=seq_len)['input_ids'].cuda()\n",
    "output = decode(transformer, input_ids)\n",
    "\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets a bit better when you train on 10,000 examples for 20,000 steps, but that roughly takes one hour on a labtop GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" The\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Visualize Attentions\n",
    "\n",
    "Now that we understand the basic mechanisms of attention, we can check the activated attention patterns in a pretrained BERT model (Devlin et al. 2018). Recall that BERT is an encoder-based transformer model which is based on a stack of self-attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI-bIuxUteuC",
    "outputId": "40ee1d8b-a37f-4c58-dbcb-6835d83c298a"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# Define a sample input text\n",
    "text = \"I will go for a run and will jump into a lake.\"\n",
    "\n",
    "# Instantiate the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = [1] * len(token_ids)\n",
    "\n",
    "# Convert token IDs and attention mask to tensors\n",
    "input_ids = torch.tensor([token_ids])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "# Generate the transformer output\n",
    "outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "\n",
    "# Extract attentions and check the shape\n",
    "outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we extracted an attention from the first layer. The first dimension is the bach, the second one is the number of heads used in the first layer, and the last two dimensions are the sequence length. Given that this was a self attention block the last two numbers are equal.\n",
    "\n",
    "We can now use a method from the [bertviz library](https://github.com/jessevig/bertviz) and plot all the heads.\n",
    "\n",
    "You'll see a dropdown menu that allows you the select a layer of the model (GPT-2 has 12). You'll then see a color for every head used in that layer (GPT-2 has 12 head per layer). By default all heads are shown, click on a color to activate/disactivate that head. It can help starting by activating only one head and checking the learned relation learn by that self attentino head. By hovering over each word you can see the attention weigths that linked that words to all the others.\n",
    "\n",
    "**Question** Do you notice any interesting (linguistic) pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(outputs.attentions, tokens=tokens)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GAQAv4iil7LX"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "004553b714df4f9fa94e3e8aad429cb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00d8c994da6041949554985cc8425e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06bae1d9afcb42f79e8c9447f34568df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b49db4be6db4ca0aca576d90796e498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10dd6e3f02dd4661abfaaf75b43e2914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c46a02021334c9dafa7e3cbea7c3fd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ec5a1f535934e2ba6c4f67357ab97b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "208fa2e9acc8443698cf6f1cd0f7bf82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2584967c2b7b4821a8aa1ae1eaa602b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdfb7a69f3524fd3a66de434c0e03ecc",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63e549e71a2b425dba863727737f53bb",
      "value": 570
     }
    },
    "30aba7c8bea241efb630266330eaec29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ec5a1f535934e2ba6c4f67357ab97b4",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50f169d401e348b486e15a68b9f0bebf",
      "value": 231508
     }
    },
    "3c28f8163719481ba3d4cc34cb055060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56e04d73c266491ebe0623f69f9ce763",
      "placeholder": "​",
      "style": "IPY_MODEL_208fa2e9acc8443698cf6f1cd0f7bf82",
      "value": " 440M/440M [00:04&lt;00:00, 103MB/s]"
     }
    },
    "411c6dab66b54654a0405f4df08c5639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de646d633a914d9592090f65051a62ff",
      "placeholder": "​",
      "style": "IPY_MODEL_00d8c994da6041949554985cc8425e8e",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "481efe6d7c3a46d08a37619d8cd5809b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "484405deb40f462f81e82e6139d77fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd02d3c7f1b54926be6f7599455fb024",
      "placeholder": "​",
      "style": "IPY_MODEL_0b49db4be6db4ca0aca576d90796e498",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "50f169d401e348b486e15a68b9f0bebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56e04d73c266491ebe0623f69f9ce763": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e549e71a2b425dba863727737f53bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "652d57b1fcc6438c993c8ef6fc0a14fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70381041f4544b04bb3bb9e49c8defc5",
      "placeholder": "​",
      "style": "IPY_MODEL_10dd6e3f02dd4661abfaaf75b43e2914",
      "value": " 28.0/28.0 [00:00&lt;00:00, 276B/s]"
     }
    },
    "67aef5b516a54f73bd18947635221ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_686bbfa0b07e449f84da2d23328aed52",
      "placeholder": "​",
      "style": "IPY_MODEL_8fa6940c2a1047f2a364bff4abc0776e",
      "value": " 570/570 [00:00&lt;00:00, 8.40kB/s]"
     }
    },
    "686bbfa0b07e449f84da2d23328aed52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69384a586dfb4bdcbe22ab9d66443ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70381041f4544b04bb3bb9e49c8defc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77e4148a5eff405a93f81cdf131daf3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a198c7be8974598a61e2f231502e2f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b983da7cd8f24ee4a3e2ee378c8e8d3b",
       "IPY_MODEL_cd2c2b9d0e2a4144970196a470ff6437",
       "IPY_MODEL_3c28f8163719481ba3d4cc34cb055060"
      ],
      "layout": "IPY_MODEL_004553b714df4f9fa94e3e8aad429cb4"
     }
    },
    "8fa6940c2a1047f2a364bff4abc0776e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac296dbdef4d40d38e7fdec2ebcd5382": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8bf52de9eae4860bd5c8a3c6d6e4bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b983da7cd8f24ee4a3e2ee378c8e8d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77e4148a5eff405a93f81cdf131daf3b",
      "placeholder": "​",
      "style": "IPY_MODEL_69384a586dfb4bdcbe22ab9d66443ab5",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "cd2c2b9d0e2a4144970196a470ff6437": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_481efe6d7c3a46d08a37619d8cd5809b",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1130befba034636a0173fec87a18a92",
      "value": 440449768
     }
    },
    "cdfb7a69f3524fd3a66de434c0e03ecc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0a6f5622ecb4ef4a38c1ce90f15c072": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6381879001341a18187c6e63f10c464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d962809a60f64f8b9097a98fce3ea48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd02d3c7f1b54926be6f7599455fb024": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de646d633a914d9592090f65051a62ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1130befba034636a0173fec87a18a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e5f17745469147cd9233eeb8b5b8461d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_484405deb40f462f81e82e6139d77fd6",
       "IPY_MODEL_fad47dcd32844881bb2b07606c54cd6d",
       "IPY_MODEL_652d57b1fcc6438c993c8ef6fc0a14fd"
      ],
      "layout": "IPY_MODEL_06bae1d9afcb42f79e8c9447f34568df"
     }
    },
    "e6e49d02bd8042648d91a81c1e7b06f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e728ccc9ef7a4ad78d2d64308495c322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6e49d02bd8042648d91a81c1e7b06f2",
      "placeholder": "​",
      "style": "IPY_MODEL_d6381879001341a18187c6e63f10c464",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "eb7badb8586b466c9ea01d163c45e820": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eba9114e27f54e7f9f5926971e83de2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e728ccc9ef7a4ad78d2d64308495c322",
       "IPY_MODEL_30aba7c8bea241efb630266330eaec29",
       "IPY_MODEL_ecf82a9385c74c57811a5390693b475c"
      ],
      "layout": "IPY_MODEL_eb7badb8586b466c9ea01d163c45e820"
     }
    },
    "ecf82a9385c74c57811a5390693b475c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac296dbdef4d40d38e7fdec2ebcd5382",
      "placeholder": "​",
      "style": "IPY_MODEL_d962809a60f64f8b9097a98fce3ea48d",
      "value": " 232k/232k [00:00&lt;00:00, 1.89MB/s]"
     }
    },
    "f3e49e667464406cbd7a283c5dc6c354": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_411c6dab66b54654a0405f4df08c5639",
       "IPY_MODEL_2584967c2b7b4821a8aa1ae1eaa602b6",
       "IPY_MODEL_67aef5b516a54f73bd18947635221ce3"
      ],
      "layout": "IPY_MODEL_b8bf52de9eae4860bd5c8a3c6d6e4bb6"
     }
    },
    "fad47dcd32844881bb2b07606c54cd6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c46a02021334c9dafa7e3cbea7c3fd9",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0a6f5622ecb4ef4a38c1ce90f15c072",
      "value": 28
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
